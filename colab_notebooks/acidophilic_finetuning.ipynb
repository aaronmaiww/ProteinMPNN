{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProteinMPNN Finetuning with Acidophilic Dataset\n",
    "\n",
    "This notebook demonstrates how to finetune ProteinMPNN using the acidophilic dataset. It includes all the necessary steps, from data preparation to model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's make sure we have all the required packages installed and set up our working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab and install dependencies if needed\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone the repo if in Colab\n",
    "    !git clone https://github.com/aaronmaiww/ProteinMPNN.git\n",
    "    %cd ProteinMPNN\n",
    "    !git checkout finetuning-modification\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install torch==2.0.1 numpy dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths and check directory structure\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If not in Colab, assume we're in the ProteinMPNN directory\n",
    "if not IN_COLAB:\n",
    "    # Add the current directory to the path\n",
    "    sys.path.append(os.getcwd())\n",
    "else:\n",
    "    # In Colab, we're already in the right directory\n",
    "    pass\n",
    "\n",
    "# Import necessary modules\n",
    "from training.utils import PDB_dataset, StructureDataset, StructureLoader, worker_init_fn\n",
    "from training.model_utils import featurize, loss_smoothed, loss_nll, get_std_opt, ProteinMPNN\n",
    "\n",
    "# Display directory structure\n",
    "!ls -la training/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Extract Acidophilic Dataset\n",
    "\n",
    "Now, let's download and extract the acidophilic dataset. This assumes the dataset is available in your GitHub repository or another accessible location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory for the dataset\n",
    "!mkdir -p /tmp/acidophilic_data\n",
    "\n",
    "# For demonstration purposes - in a real notebook, you'd download from a URL\n",
    "if IN_COLAB:\n",
    "    # In Colab, you might need to download the dataset from a URL\n",
    "    dataset_url = \"YOUR_DATASET_URL_HERE\"\n",
    "    !wget -O /tmp/acidophilic_dataset_converted.zip {dataset_url}\n",
    "    !unzip -q /tmp/acidophilic_dataset_converted.zip -d /tmp/acidophilic_data\n",
    "else:\n",
    "    # Locally, use the existing dataset\n",
    "    if os.path.exists(\"acidophilic_dataset_converted.zip\"):\n",
    "        !unzip -q acidophilic_dataset_converted.zip -d /tmp/acidophilic_data\n",
    "    else:\n",
    "        print(\"Dataset not found locally. Please download it or provide a URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset structure\n",
    "!ls -la /tmp/acidophilic_data/acidophilic_dataset_for_aaron_converted\n",
    "\n",
    "# Display a sample of the training proteins\n",
    "!head -n 10 /tmp/acidophilic_data/acidophilic_dataset_for_aaron_converted/train_proteins.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Custom Functions for Acidophilic Dataset\n",
    "\n",
    "Next, let's define the custom functions needed to work with the acidophilic dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "\n",
    "def build_custom_training_clusters(params, debug=False):\n",
    "    \"\"\"Build training clusters from protein lists instead of list.csv\"\"\"\n",
    "    # Read protein lists\n",
    "    with open(params['VAL'], 'r') as f:\n",
    "        val_proteins = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    with open(params['TEST'], 'r') as f:\n",
    "        test_proteins = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    train_file = os.path.join(params['DIR'], 'train_proteins.txt')\n",
    "    with open(train_file, 'r') as f:\n",
    "        train_proteins = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    print(f\"Found {len(train_proteins)} train proteins, {len(val_proteins)} validation proteins, {len(test_proteins)} test proteins\")\n",
    "    \n",
    "    # Create dictionaries in the expected format\n",
    "    train = {}\n",
    "    valid = {}\n",
    "    test = {}\n",
    "    \n",
    "    # For train proteins, add them to train dict\n",
    "    for protein in train_proteins:\n",
    "        if debug and len(train) >= 20:  # Limit in debug mode\n",
    "            break\n",
    "        train[len(train)] = [[f\"{protein}_A\", protein]]\n",
    "        \n",
    "    # For validation proteins, add them to valid dict\n",
    "    for protein in val_proteins:\n",
    "        if debug and len(valid) >= 10:  # Limit in debug mode\n",
    "            break\n",
    "        valid[len(valid)] = [[f\"{protein}_A\", protein]]\n",
    "        \n",
    "    # For test proteins, add them to test dict\n",
    "    for protein in test_proteins:\n",
    "        if debug and len(test) >= 10:  # Limit in debug mode\n",
    "            break\n",
    "        test[len(test)] = [[f\"{protein}_A\", protein]]\n",
    "        \n",
    "    if debug:\n",
    "        print(\"Train sample:\", list(train.items())[:5])\n",
    "        print(\"Valid sample:\", list(valid.items())[:5])\n",
    "        \n",
    "    return train, valid, test\n",
    "\n",
    "def custom_loader_pdb(item, params):\n",
    "    \"\"\"Custom loader for acidophilic dataset structure\"\"\"\n",
    "    pdbid, chid = item[0].split('_')\n",
    "    \n",
    "    # Determine if it's a train, validation, or test protein\n",
    "    dataset_type = None\n",
    "    train_file = os.path.join(params['DIR'], 'train_proteins.txt')\n",
    "    val_file = os.path.join(params['DIR'], 'validation_proteins.txt')\n",
    "    test_file = os.path.join(params['DIR'], 'test_proteins.txt')\n",
    "    \n",
    "    with open(train_file, 'r') as f:\n",
    "        train_proteins = [line.strip() for line in f.readlines()]\n",
    "    with open(val_file, 'r') as f:\n",
    "        val_proteins = [line.strip() for line in f.readlines()]\n",
    "    with open(test_file, 'r') as f:\n",
    "        test_proteins = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    if pdbid in train_proteins:\n",
    "        dataset_type = \"train\"\n",
    "    elif pdbid in val_proteins:\n",
    "        dataset_type = \"validation\"\n",
    "    elif pdbid in test_proteins:\n",
    "        dataset_type = \"test\"\n",
    "    else:\n",
    "        print(f\"Protein {pdbid} not found in any dataset!\")\n",
    "        return {'seq': np.zeros(5)}\n",
    "    \n",
    "    # Extract first two characters for directory structure\n",
    "    first_two = pdbid[:2]\n",
    "    \n",
    "    PREFIX = f\"{params['DIR']}/{dataset_type}/pdb/{first_two}/{pdbid}/{pdbid}\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.isfile(f\"{PREFIX}.pt\"):\n",
    "        print(f\"File not found: {PREFIX}.pt\")\n",
    "        return {'seq': np.zeros(5)}\n",
    "    \n",
    "    # Load metadata\n",
    "    meta = torch.load(f\"{PREFIX}.pt\")\n",
    "    \n",
    "    # Check if chain file exists\n",
    "    chain_file = f\"{PREFIX}_{chid}.pt\"\n",
    "    if not os.path.isfile(chain_file):\n",
    "        print(f\"Chain file not found: {chain_file}\")\n",
    "        return {'seq': np.zeros(5)}\n",
    "    \n",
    "    # Load chain data\n",
    "    chain = torch.load(chain_file)\n",
    "    L = len(chain['seq'])\n",
    "    \n",
    "    # Return data in the expected format\n",
    "    return {'seq': chain['seq'],\n",
    "            'xyz': chain['xyz'],\n",
    "            'idx': torch.zeros(L).int(),\n",
    "            'masked': torch.Tensor([0]).int(),\n",
    "            'label': item[0]}\n",
    "\n",
    "def load_data_from_loader(data_loader, max_length, num_examples):\n",
    "    \"\"\"Load data directly from loader_pdb output and convert to expected format\"\"\"\n",
    "    pdb_dict_list = []\n",
    "    count = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        # Handle the batch format - extract values from lists\n",
    "        # batch contains lists because DataLoader wraps everything in batches\n",
    "        \n",
    "        # Extract single values from batch lists\n",
    "        label = batch['label'][0] if isinstance(batch['label'], list) else batch['label']\n",
    "        seq = batch['seq'][0] if isinstance(batch['seq'], list) else batch['seq']\n",
    "        xyz = batch['xyz']\n",
    "        idx = batch['idx'][0] if batch['idx'].dim() > 1 else batch['idx']\n",
    "        masked = batch['masked'][0] if batch['masked'].dim() > 1 else batch['masked']\n",
    "        \n",
    "        print(f\"DEBUG: Processing {label}, seq_len={len(seq)}, xyz_shape={xyz.shape}\")\n",
    "        \n",
    "        if len(seq) <= max_length:\n",
    "            # Convert to format expected by featurize\n",
    "            chain_id = label.split('_')[-1]  # Extract chain ID (e.g., 'A')\n",
    "            \n",
    "            # Remove His-tags from sequence\n",
    "            sequence = seq\n",
    "            if sequence[-6:] == \"HHHHHH\":\n",
    "                sequence = sequence[:-6]\n",
    "            if sequence[0:6] == \"HHHHHH\":\n",
    "                sequence = sequence[6:]\n",
    "            # Add other His-tag removal patterns if needed\n",
    "            \n",
    "            if len(sequence) < 4:\n",
    "                continue\n",
    "                \n",
    "            # Get coordinates and ensure proper shape\n",
    "            # Handle various possible shapes of coordinates\n",
    "            if xyz.dim() == 4 and xyz.shape[0] == 1 and xyz.shape[2] == 4:  # [1, L, 4, 3]\n",
    "                all_atoms = xyz.squeeze(0)\n",
    "            elif xyz.dim() == 5 and xyz.shape[0] == 1 and xyz.shape[1] == 1:  # [1, 1, L, 4, 3]\n",
    "                all_atoms = xyz.squeeze(0).squeeze(0)\n",
    "            else:\n",
    "                print(f\"Unable to handle coordinate shape: {xyz.shape}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"DEBUG: {label} - reshaped coords to shape: {all_atoms.shape}, seq length: {len(sequence)}\")\n",
    "            \n",
    "            # Adjust coordinates to match sequence length after His-tag removal\n",
    "            if len(all_atoms) > len(sequence):\n",
    "                print(f\"DEBUG: Trimming coordinates from {len(all_atoms)} to {len(sequence)}\")\n",
    "                all_atoms = all_atoms[:len(sequence)]\n",
    "            elif len(all_atoms) < len(sequence):\n",
    "                print(f\"Warning: coordinates shorter than sequence for {label}: coords={len(all_atoms)}, seq={len(sequence)}\")\n",
    "                continue\n",
    "            \n",
    "            # Create coordinate dictionary\n",
    "            coords_dict = {\n",
    "                f'N_chain_{chain_id}': all_atoms[:,0,:].tolist(),\n",
    "                f'CA_chain_{chain_id}': all_atoms[:,1,:].tolist(),\n",
    "                f'C_chain_{chain_id}': all_atoms[:,2,:].tolist(),\n",
    "                f'O_chain_{chain_id}': all_atoms[:,3,:].tolist(),\n",
    "            }\n",
    "            \n",
    "            # Determine masking\n",
    "            if masked.dim() > 0:\n",
    "                masked_values = masked.tolist()\n",
    "            else:\n",
    "                masked_values = [masked.item()]\n",
    "            \n",
    "            if 0 in masked_values:\n",
    "                masked_list = [chain_id]\n",
    "                visible_list = []\n",
    "            else:\n",
    "                masked_list = []\n",
    "                visible_list = [chain_id]\n",
    "            \n",
    "            # Create final structure in format expected by featurize\n",
    "            converted_item = {\n",
    "                f'seq_chain_{chain_id}': sequence,\n",
    "                f'coords_chain_{chain_id}': coords_dict,\n",
    "                'masked_list': masked_list,\n",
    "                'visible_list': visible_list,\n",
    "                'num_of_chains': 1,\n",
    "                'seq': sequence,\n",
    "                'name': label\n",
    "            }\n",
    "            \n",
    "            pdb_dict_list.append(converted_item)\n",
    "            count += 1\n",
    "            print(f\"DEBUG: Successfully converted {label}\")\n",
    "            \n",
    "            if count >= num_examples:\n",
    "                break\n",
    "    \n",
    "    return pdb_dict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model\n",
    "\n",
    "Now, let's set up the training parameters and start the finetuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.path_for_training_data = \"/tmp/acidophilic_data/acidophilic_dataset_for_aaron_converted\"\n",
    "        self.path_for_outputs = \"./notebook_finetuning_%Y%m%d_%H%M%S\"\n",
    "        self.previous_checkpoint = \"\"  # Optional: path to previous checkpoint\n",
    "        self.num_epochs = 5  # Set to a small number for demonstration\n",
    "        self.save_model_every_n_epochs = 1\n",
    "        self.reload_data_every_n_epochs = 2\n",
    "        self.num_examples_per_epoch = 50  # Small number for demonstration\n",
    "        self.batch_size = 1000\n",
    "        self.max_protein_length = 1000\n",
    "        self.hidden_dim = 128\n",
    "        self.num_encoder_layers = 3\n",
    "        self.num_decoder_layers = 3\n",
    "        self.num_neighbors = 48\n",
    "        self.dropout = 0.1\n",
    "        self.backbone_noise = 0.2\n",
    "        self.rescut = 3.5\n",
    "        self.debug = True  # Set to True for faster debugging\n",
    "        self.gradient_norm = -1.0\n",
    "        self.mixed_precision = True\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize mixed precision if needed\n",
    "scaler = torch.cuda.amp.GradScaler() if args.mixed_precision else None\n",
    "\n",
    "# Set up output folder\n",
    "base_folder = time.strftime(args.path_for_outputs, time.localtime())\n",
    "if base_folder[-1] != '/':\n",
    "    base_folder += '/'\n",
    "if not os.path.exists(base_folder):\n",
    "    os.makedirs(base_folder)\n",
    "subfolders = ['model_weights']\n",
    "for subfolder in subfolders:\n",
    "    if not os.path.exists(base_folder + subfolder):\n",
    "        os.makedirs(base_folder + subfolder)\n",
    "\n",
    "# Set up logging\n",
    "logfile = base_folder + 'log.txt'\n",
    "with open(logfile, 'w') as f:\n",
    "    f.write('Epoch\\tTrain\\tValidation\\n')\n",
    "\n",
    "# Set up data parameters\n",
    "data_path = args.path_for_training_data\n",
    "params = {\n",
    "    \"VAL\": f\"{data_path}/validation_proteins.txt\",\n",
    "    \"TEST\": f\"{data_path}/test_proteins.txt\",\n",
    "    \"DIR\": f\"{data_path}\",\n",
    "    \"DATCUT\": \"2030-Jan-01\",\n",
    "    \"RESCUT\": args.rescut,\n",
    "    \"HOMO\": 0.70\n",
    "}\n",
    "\n",
    "# DataLoader parameters\n",
    "LOAD_PARAM = {\n",
    "    'batch_size': 1,\n",
    "    'shuffle': True,\n",
    "    'pin_memory': False,\n",
    "    'num_workers': 2  # Reduced for Jupyter\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training clusters and create datasets\n",
    "print(\"Building training clusters...\")\n",
    "train, valid, test = build_custom_training_clusters(params, args.debug)\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "train_set = PDB_dataset(list(train.keys()), custom_loader_pdb, train, params)\n",
    "valid_set = PDB_dataset(list(valid.keys()), custom_loader_pdb, valid, params)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, worker_init_fn=worker_init_fn, **LOAD_PARAM)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, worker_init_fn=worker_init_fn, **LOAD_PARAM)\n",
    "\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Validation set size: {len(valid_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = ProteinMPNN(node_features=args.hidden_dim, \n",
    "                edge_features=args.hidden_dim, \n",
    "                hidden_dim=args.hidden_dim, \n",
    "                num_encoder_layers=args.num_encoder_layers, \n",
    "                num_decoder_layers=args.num_encoder_layers, \n",
    "                k_neighbors=args.num_neighbors, \n",
    "                dropout=args.dropout, \n",
    "                augment_eps=args.backbone_noise)\n",
    "model.to(device)\n",
    "\n",
    "# Load pre-trained weights if specified\n",
    "if args.previous_checkpoint:\n",
    "    print(f\"Loading weights from {args.previous_checkpoint}\")\n",
    "    checkpoint = torch.load(args.previous_checkpoint)\n",
    "    total_step = checkpoint['step']\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    optimizer = get_std_opt(model.parameters(), args.hidden_dim, total_step)\n",
    "    if 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "else:\n",
    "    total_step = 0\n",
    "    epoch = 0\n",
    "    optimizer = get_std_opt(model.parameters(), args.hidden_dim, total_step)\n",
    "\n",
    "print(\"Model created and initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "print(\"Loading training data...\")\n",
    "pdb_dict_train = load_data_from_loader(train_loader, args.max_protein_length, args.num_examples_per_epoch)\n",
    "print(f\"Loaded {len(pdb_dict_train)} training examples\")\n",
    "\n",
    "print(\"\\nLoading validation data...\")\n",
    "pdb_dict_valid = load_data_from_loader(valid_loader, args.max_protein_length, args.num_examples_per_epoch)\n",
    "print(f\"Loaded {len(pdb_dict_valid)} validation examples\")\n",
    "\n",
    "# Create structure datasets\n",
    "dataset_train = StructureDataset(pdb_dict_train, truncate=None, max_length=args.max_protein_length)\n",
    "dataset_valid = StructureDataset(pdb_dict_valid, truncate=None, max_length=args.max_protein_length)\n",
    "\n",
    "# Create structure loaders\n",
    "loader_train = StructureLoader(dataset_train, batch_size=args.batch_size)\n",
    "loader_valid = StructureLoader(dataset_valid, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "for e in range(args.num_epochs):\n",
    "    t0 = time.time()\n",
    "    e = epoch + e\n",
    "    model.train()\n",
    "    train_sum, train_weights = 0., 0.\n",
    "    train_acc = 0.\n",
    "    \n",
    "    print(f\"Epoch {e+1}/{epoch + args.num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    for batch_idx, batch in enumerate(loader_train):\n",
    "        print(f\"  Training batch {batch_idx+1}/{len(loader_train)}\")\n",
    "        X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "        mask_for_loss = mask*chain_M\n",
    "        \n",
    "        if args.mixed_precision:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)\n",
    "                _, loss_av_smoothed = loss_smoothed(S, log_probs, mask_for_loss)\n",
    "    \n",
    "            scaler.scale(loss_av_smoothed).backward()\n",
    "              \n",
    "            if args.gradient_norm > 0.0:\n",
    "                total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.gradient_norm)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)\n",
    "            _, loss_av_smoothed = loss_smoothed(S, log_probs, mask_for_loss)\n",
    "            loss_av_smoothed.backward()\n",
    "\n",
    "            if args.gradient_norm > 0.0:\n",
    "                total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.gradient_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Calculate loss and accuracy\n",
    "        loss, loss_av, true_false = loss_nll(S, log_probs, mask_for_loss)\n",
    "        train_sum += torch.sum(loss * mask_for_loss).cpu().data.numpy()\n",
    "        train_acc += torch.sum(true_false * mask_for_loss).cpu().data.numpy()\n",
    "        train_weights += torch.sum(mask_for_loss).cpu().data.numpy()\n",
    "        \n",
    "        total_step += 1\n",
    "    \n",
    "    # Validation\n",
    "    print(\"  Running validation...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_sum, validation_weights = 0., 0.\n",
    "        validation_acc = 0.\n",
    "        for batch_idx, batch in enumerate(loader_valid):\n",
    "            X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)\n",
    "            log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)\n",
    "            mask_for_loss = mask*chain_M\n",
    "            loss, loss_av, true_false = loss_nll(S, log_probs, mask_for_loss)\n",
    "            \n",
    "            validation_sum += torch.sum(loss * mask_for_loss).cpu().data.numpy()\n",
    "            validation_acc += torch.sum(true_false * mask_for_loss).cpu().data.numpy()\n",
    "            validation_weights += torch.sum(mask_for_loss).cpu().data.numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_loss = train_sum / train_weights if train_weights > 0 else float('inf')\n",
    "    train_accuracy = train_acc / train_weights if train_weights > 0 else 0\n",
    "    train_perplexity = np.exp(train_loss)\n",
    "    validation_loss = validation_sum / validation_weights if validation_weights > 0 else float('inf')\n",
    "    validation_accuracy = validation_acc / validation_weights if validation_weights > 0 else 0\n",
    "    validation_perplexity = np.exp(validation_loss)\n",
    "    \n",
    "    # Format metrics for printing\n",
    "    train_perplexity_ = np.format_float_positional(np.float32(train_perplexity), unique=False, precision=3)     \n",
    "    validation_perplexity_ = np.format_float_positional(np.float32(validation_perplexity), unique=False, precision=3)\n",
    "    train_accuracy_ = np.format_float_positional(np.float32(train_accuracy), unique=False, precision=3)\n",
    "    validation_accuracy_ = np.format_float_positional(np.float32(validation_accuracy), unique=False, precision=3)\n",
    "\n",
    "    # Calculate time and log results\n",
    "    t1 = time.time()\n",
    "    dt = np.format_float_positional(np.float32(t1-t0), unique=False, precision=1) \n",
    "    \n",
    "    # Log results\n",
    "    log_message = f'epoch: {e+1}, step: {total_step}, time: {dt}, train: {train_perplexity_}, valid: {validation_perplexity_}, train_acc: {train_accuracy_}, valid_acc: {validation_accuracy_}'\n",
    "    with open(logfile, 'a') as f:\n",
    "        f.write(f'{log_message}\\n')\n",
    "    print(log_message)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_filename_last = base_folder+'model_weights/epoch_last.pt'\n",
    "    torch.save({\n",
    "                'epoch': e+1,\n",
    "                'step': total_step,\n",
    "                'num_edges' : args.num_neighbors,\n",
    "                'noise_level': args.backbone_noise,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.optimizer.state_dict(),\n",
    "                }, checkpoint_filename_last)\n",
    "\n",
    "    # Save periodic checkpoints\n",
    "    if (e+1) % args.save_model_every_n_epochs == 0:\n",
    "        checkpoint_filename = base_folder+f'model_weights/epoch{e+1}_step{total_step}.pt'\n",
    "        torch.save({\n",
    "                'epoch': e+1,\n",
    "                'step': total_step,\n",
    "                'num_edges' : args.num_neighbors,\n",
    "                'noise_level': args.backbone_noise, \n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.optimizer.state_dict(),\n",
    "                }, checkpoint_filename)\n",
    "\n",
    "    # Reload data periodically if needed\n",
    "    if (e+1) % args.reload_data_every_n_epochs == 0 and e+1 < epoch + args.num_epochs:\n",
    "        print(\"Reloading training data...\")\n",
    "        pdb_dict_train = load_data_from_loader(train_loader, args.max_protein_length, args.num_examples_per_epoch)\n",
    "        dataset_train = StructureDataset(pdb_dict_train, truncate=None, max_length=args.max_protein_length)\n",
    "        loader_train = StructureLoader(dataset_train, batch_size=args.batch_size)\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Model\n",
    "\n",
    "Let's evaluate the trained model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest model for evaluation\n",
    "latest_checkpoint = torch.load(checkpoint_filename_last)\n",
    "model.load_state_dict(latest_checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Run a comprehensive evaluation on the validation set\n",
    "print(\"Running comprehensive evaluation on validation set...\")\n",
    "with torch.no_grad():\n",
    "    all_losses = []\n",
    "    all_accuracies = []\n",
    "    all_perplexities = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(loader_valid):\n",
    "        X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)\n",
    "        log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)\n",
    "        mask_for_loss = mask*chain_M\n",
    "        loss, loss_av, true_false = loss_nll(S, log_probs, mask_for_loss)\n",
    "        \n",
    "        # Calculate metrics per batch\n",
    "        batch_loss = torch.sum(loss * mask_for_loss).cpu().data.numpy() / torch.sum(mask_for_loss).cpu().data.numpy()\n",
    "        batch_accuracy = torch.sum(true_false * mask_for_loss).cpu().data.numpy() / torch.sum(mask_for_loss).cpu().data.numpy()\n",
    "        batch_perplexity = np.exp(batch_loss)\n",
    "        \n",
    "        all_losses.append(batch_loss)\n",
    "        all_accuracies.append(batch_accuracy)\n",
    "        all_perplexities.append(batch_perplexity)\n",
    "        \n",
    "        print(f\"Batch {batch_idx}: Loss = {batch_loss:.4f}, Accuracy = {batch_accuracy:.4f}, Perplexity = {batch_perplexity:.4f}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_loss = np.mean(all_losses)\n",
    "    avg_accuracy = np.mean(all_accuracies)\n",
    "    avg_perplexity = np.mean(all_perplexities)\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Average Perplexity: {avg_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Progress\n",
    "\n",
    "Let's visualize the training progress using the log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Parse log file\n",
    "epochs = []\n",
    "train_perplexities = []\n",
    "valid_perplexities = []\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "\n",
    "with open(logfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:  # Skip header\n",
    "        match = re.match(r'epoch: (\\d+), step: (\\d+), time: ([\\d.]+), train: ([\\d.]+), valid: ([\\d.]+), train_acc: ([\\d.]+), valid_acc: ([\\d.]+)', line)\n",
    "        if match:\n",
    "            epoch, _, _, train_perp, valid_perp, train_acc, valid_acc = match.groups()\n",
    "            epochs.append(int(epoch))\n",
    "            train_perplexities.append(float(train_perp))\n",
    "            valid_perplexities.append(float(valid_perp))\n",
    "            train_accuracies.append(float(train_acc))\n",
    "            valid_accuracies.append(float(valid_acc))\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot perplexity\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(epochs, train_perplexities, 'b-', label='Training Perplexity')\n",
    "plt.plot(epochs, valid_perplexities, 'r-', label='Validation Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Training and Validation Perplexity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
    "plt.plot(epochs, valid_accuracies, 'r-', label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_folder + 'training_progress.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model for Later Use\n",
    "\n",
    "Let's save the final model in a format that can be easily loaded for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = base_folder + 'model_weights/final_model.pt'\n",
    "torch.save({\n",
    "    'epoch': epoch + args.num_epochs,\n",
    "    'step': total_step,\n",
    "    'num_edges': args.num_neighbors,\n",
    "    'noise_level': args.backbone_noise,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"Final model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using the Trained Model for Inference\n",
    "\n",
    "Here's a simple example of how to use the trained model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "from training.model_utils import ProteinMPNN\n",
    "\n",
    "# Create a new model with the same architecture\n",
    "inference_model = ProteinMPNN(node_features=args.hidden_dim, \n",
    "                          edge_features=args.hidden_dim, \n",
    "                          hidden_dim=args.hidden_dim, \n",
    "                          num_encoder_layers=args.num_encoder_layers, \n",
    "                          num_decoder_layers=args.num_encoder_layers, \n",
    "                          k_neighbors=args.num_neighbors, \n",
    "                          dropout=0.0,  # No dropout during inference \n",
    "                          augment_eps=0.0)  # No noise during inference\n",
    "inference_model.to(device)\n",
    "\n",
    "# Load the weights from the trained model\n",
    "checkpoint = torch.load(final_model_path)\n",
    "inference_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "inference_model.eval()\n",
    "\n",
    "print(\"Model loaded for inference.\")\n",
    "\n",
    "# You can now use the model for inference with your custom inputs\n",
    "# Example: inference_model(X, S, mask, chain_M, residue_idx, chain_encoding_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to finetune ProteinMPNN using the acidophilic dataset. We covered:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Loading and preprocessing the acidophilic dataset\n",
    "3. Implementing custom functions for the dataset\n",
    "4. Training the model\n",
    "5. Evaluating the model performance\n",
    "6. Visualizing the training progress\n",
    "7. Saving and loading the model for inference\n",
    "\n",
    "This finetuned model can now be used for protein sequence design tasks specifically tailored to acidophilic proteins."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}