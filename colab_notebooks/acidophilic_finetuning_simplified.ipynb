{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProteinMPNN Finetuning with Acidophilic Dataset - Simplified\n",
    "\n",
    "This notebook demonstrates how to finetune ProteinMPNN using the acidophilic dataset using the modular functions from finetuning.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's make sure we have all the required packages installed and set up our working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab and install dependencies if needed\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone the repo if in Colab\n",
    "    !git clone https://github.com/aaronmaiww/ProteinMPNN.git\n",
    "    %cd ProteinMPNN\n",
    "    !git checkout finetuning-modification\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install torch==2.0.1 numpy dateutil\n",
    "else:\n",
    "    # Add the current directory to the path if not in Colab\n",
    "    sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Extract Acidophilic Dataset\n",
    "\n",
    "Let's download and extract the acidophilic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory for the dataset\n",
    "!mkdir -p /tmp/acidophilic_data\n",
    "\n",
    "if IN_COLAB:\n",
    "    # In Colab, you might need to download the dataset from a URL\n",
    "    dataset_url = \"YOUR_DATASET_URL_HERE\"\n",
    "    !wget -O /tmp/acidophilic_dataset_converted.zip {dataset_url}\n",
    "    !unzip -q /tmp/acidophilic_dataset_converted.zip -d /tmp/acidophilic_data\n",
    "else:\n",
    "    # Locally, use the existing dataset\n",
    "    if os.path.exists(\"acidophilic_dataset_converted.zip\"):\n",
    "        !unzip -q acidophilic_dataset_converted.zip -d /tmp/acidophilic_data\n",
    "    else:\n",
    "        print(\"Dataset not found locally. Please download it or provide a URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset structure\n",
    "!ls -la /tmp/acidophilic_data/acidophilic_dataset_for_aaron_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Required Functions\n",
    "\n",
    "Now, let's import the functions we need from the training module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from finetuning.py\n",
    "from training.finetuning import setup_acidophilic_finetuning, train_and_validate, run_acidophilic_finetuning\n",
    "from training.utils import StructureDataset, StructureLoader\n",
    "from training.model_utils import ProteinMPNN\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Training Parameters\n",
    "\n",
    "Let's set up the training parameters for our finetuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters as a class (for compatibility with the functions)\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.path_for_training_data = \"/tmp/acidophilic_data/acidophilic_dataset_for_aaron_converted\"\n",
    "        self.path_for_outputs = \"./notebook_finetuning_%Y%m%d_%H%M%S\"\n",
    "        self.previous_checkpoint = \"\"  # Optional: path to previous checkpoint\n",
    "        self.num_epochs = 5  # Set to a small number for demonstration\n",
    "        self.save_model_every_n_epochs = 1\n",
    "        self.reload_data_every_n_epochs = 2\n",
    "        self.num_examples_per_epoch = 50  # Small number for demonstration\n",
    "        self.batch_size = 1000\n",
    "        self.max_protein_length = 1000\n",
    "        self.hidden_dim = 128\n",
    "        self.num_encoder_layers = 3\n",
    "        self.num_decoder_layers = 3\n",
    "        self.num_neighbors = 48\n",
    "        self.dropout = 0.1\n",
    "        self.backbone_noise = 0.2\n",
    "        self.rescut = 3.5\n",
    "        self.debug = True  # Set to True for faster debugging\n",
    "        self.gradient_norm = -1.0\n",
    "        self.mixed_precision = True\n",
    "        self.verbose = True\n",
    "        self.num_workers = 2  # Reduced for Jupyter\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training - Option 1: Using run_acidophilic_finetuning\n",
    "\n",
    "The simplest way to train is to use the `run_acidophilic_finetuning` function which handles the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback for logging and visualization (optional)\n",
    "training_history = {\n",
    "    'epochs': [],\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'train_perplexity': [],\n",
    "    'valid_loss': [],\n",
    "    'valid_accuracy': [],\n",
    "    'valid_perplexity': []\n",
    "}\n",
    "\n",
    "def history_callback(epoch, train_metrics, valid_metrics, model, optimizer):\n",
    "    training_history['epochs'].append(epoch + 1)\n",
    "    training_history['train_loss'].append(train_metrics['loss'])\n",
    "    training_history['train_accuracy'].append(train_metrics['accuracy'])\n",
    "    training_history['train_perplexity'].append(train_metrics['perplexity'])\n",
    "    training_history['valid_loss'].append(valid_metrics['loss'])\n",
    "    training_history['valid_accuracy'].append(valid_metrics['accuracy'])\n",
    "    training_history['valid_perplexity'].append(valid_metrics['perplexity'])\n",
    "    \n",
    "    # You could add more functionality here, like early stopping\n",
    "\n",
    "# Run the training\n",
    "results = run_acidophilic_finetuning(args, callbacks=[history_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training - Option 2: Step-by-Step Approach\n",
    "\n",
    "Alternatively, you can use a more granular approach for more control over the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "setup = setup_acidophilic_finetuning(args)\n",
    "\n",
    "model = setup['model']\n",
    "optimizer = setup['optimizer']\n",
    "scaler = setup['scaler']\n",
    "train_loader = setup['train_loader']\n",
    "valid_loader = setup['valid_loader']\n",
    "loader_train = setup['loader_train']\n",
    "loader_valid = setup['loader_valid']\n",
    "device = setup['device']\n",
    "base_folder = setup['base_folder']\n",
    "logfile = setup['logfile']\n",
    "total_step = setup['total_step']\n",
    "epoch = setup['epoch']\n",
    "params = setup['params']\n",
    "\n",
    "# Create storage for metrics\n",
    "step_by_step_history = {\n",
    "    'epochs': [],\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'train_perplexity': [],\n",
    "    'valid_loss': [],\n",
    "    'valid_accuracy': [],\n",
    "    'valid_perplexity': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "for e in range(args.num_epochs):\n",
    "    total_step, train_metrics, valid_metrics = train_and_validate(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scaler=scaler,\n",
    "        loader_train=loader_train,\n",
    "        loader_valid=loader_valid,\n",
    "        device=device,\n",
    "        epoch=epoch + e,\n",
    "        total_step=total_step,\n",
    "        base_folder=base_folder,\n",
    "        logfile=logfile,\n",
    "        args=args,\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        gradient_norm=args.gradient_norm,\n",
    "        save_checkpoints=True,\n",
    "        callbacks=None,\n",
    "        verbose=args.verbose\n",
    "    )\n",
    "    \n",
    "    # Store metrics\n",
    "    step_by_step_history['epochs'].append(epoch + e + 1)\n",
    "    step_by_step_history['train_loss'].append(train_metrics['loss'])\n",
    "    step_by_step_history['train_accuracy'].append(train_metrics['accuracy'])\n",
    "    step_by_step_history['train_perplexity'].append(train_metrics['perplexity'])\n",
    "    step_by_step_history['valid_loss'].append(valid_metrics['loss'])\n",
    "    step_by_step_history['valid_accuracy'].append(valid_metrics['accuracy'])\n",
    "    step_by_step_history['valid_perplexity'].append(valid_metrics['perplexity'])\n",
    "    \n",
    "    # Reload data if needed\n",
    "    if (e + 1) % args.reload_data_every_n_epochs == 0 and (e + 1) < args.num_epochs:\n",
    "        print(\"Reloading training data...\")\n",
    "        from training.finetuning import load_data_from_loader\n",
    "        pdb_dict_train = load_data_from_loader(train_loader, args.max_protein_length, args.num_examples_per_epoch)\n",
    "        dataset_train = StructureDataset(pdb_dict_train, truncate=None, max_length=args.max_protein_length)\n",
    "        loader_train = StructureLoader(dataset_train, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Progress\n",
    "\n",
    "Let's visualize the training progress using the metrics we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which history to visualize\n",
    "history = training_history  # or step_by_step_history\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot perplexity\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history['epochs'], history['train_perplexity'], 'b-', label='Training Perplexity')\n",
    "plt.plot(history['epochs'], history['valid_perplexity'], 'r-', label='Validation Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Training and Validation Perplexity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(history['epochs'], history['train_accuracy'], 'b-', label='Training Accuracy')\n",
    "plt.plot(history['epochs'], history['valid_accuracy'], 'r-', label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_folder + 'training_progress.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model\n",
    "\n",
    "Let's evaluate the trained model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final model\n",
    "final_model_path = base_folder + 'model_weights/final_model.pt'\n",
    "checkpoint = torch.load(final_model_path)\n",
    "\n",
    "# Create a new model for evaluation\n",
    "eval_model = ProteinMPNN(\n",
    "    node_features=args.hidden_dim,\n",
    "    edge_features=args.hidden_dim,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    num_encoder_layers=args.num_encoder_layers,\n",
    "    num_decoder_layers=args.num_encoder_layers,\n",
    "    k_neighbors=args.num_neighbors,\n",
    "    dropout=0.0,  # No dropout during evaluation\n",
    "    augment_eps=0.0  # No noise during evaluation\n",
    ")\n",
    "eval_model.to(device)\n",
    "eval_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "eval_model.eval()\n",
    "\n",
    "# Evaluate on validation set (you can reuse the train_and_validate function for this)\n",
    "from training.model_utils import featurize, loss_nll\n",
    "import torch.nn.functional as F\n",
    "\n",
    "eval_model.eval()\n",
    "with torch.no_grad():\n",
    "    validation_sum, validation_weights = 0., 0.\n",
    "    validation_acc = 0.\n",
    "    for batch_idx, batch in enumerate(loader_valid):\n",
    "        X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)\n",
    "        log_probs = eval_model(X, S, mask, chain_M, residue_idx, chain_encoding_all)\n",
    "        mask_for_loss = mask*chain_M\n",
    "        loss, loss_av, true_false = loss_nll(S, log_probs, mask_for_loss)\n",
    "        \n",
    "        validation_sum += torch.sum(loss * mask_for_loss).cpu().data.numpy()\n",
    "        validation_acc += torch.sum(true_false * mask_for_loss).cpu().data.numpy()\n",
    "        validation_weights += torch.sum(mask_for_loss).cpu().data.numpy()\n",
    "        \n",
    "        print(f\"Batch {batch_idx}: Accuracy = {torch.sum(true_false * mask_for_loss).cpu().data.numpy() / torch.sum(mask_for_loss).cpu().data.numpy():.4f}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    validation_loss = validation_sum / validation_weights\n",
    "    validation_accuracy = validation_acc / validation_weights\n",
    "    validation_perplexity = np.exp(validation_loss)\n",
    "    \n",
    "    print(\"\\nFinal Evaluation Results:\")\n",
    "    print(f\"Loss: {validation_loss:.4f}\")\n",
    "    print(f\"Accuracy: {validation_accuracy:.4f}\")\n",
    "    print(f\"Perplexity: {validation_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to finetune ProteinMPNN using the acidophilic dataset with the modular functions from finetuning.py. This approach makes the code more reusable and maintainable.\n",
    "\n",
    "The functions we used include:\n",
    "- `setup_acidophilic_finetuning`: Sets up everything needed for training\n",
    "- `train_and_validate`: Trains and validates the model for one epoch\n",
    "- `run_acidophilic_finetuning`: Runs the complete finetuning process\n",
    "\n",
    "These functions can be used in various contexts, including scripts, notebooks, and larger workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}